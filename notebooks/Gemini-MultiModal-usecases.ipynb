{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini-MultiModal-usecases.ipynb File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-requisites:\n",
    "    GCP account need to be created.\n",
    "    GCP project need to be linked with GCP billing/service account.\n",
    "    GCP project need to be linked with GCP service account key file.(json file)\n",
    "    Download json file and set env variable GOOGLE_APPLICATION_CREDENTIALS to the path of the downloaded file.\n",
    "    eg: export GOOGLE_APPLICATION_CREDENTIALS=\"/home/user/gcp_project/data/json/service-account-file.json\"\n",
    "    For different models,  ref. to model garden in Vertex AI in gcp console.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-cloud-aiplatform gitpython magika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "vertexai.init(project=\"river-span-431711-k8\", location=\"us-central1\")# Load the Gemini 1.5 Pro model. (https://cloud.google.com/vertex-ai/docs/reference/python/latest/vertexai.generative_models)\n",
    "\n",
    "# Load the Gemini 1.5 Pro model. (https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.generative_models)\n",
    "multimodal_model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "multimodal_model_flash = GenerativeModel(\"gemini-1.5-flash-001\") # using this var later\n",
    "\n",
    "# Generate response\n",
    "contents = [ \"Explain LLM\" ]\n",
    "response = multimodal_model.generate_content(contents)\n",
    "print(response)\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "display(IPython.display.Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gemini 1.5 pro model, we are going to process a PDF document. The model will analyze the document content, retain information, and provide answers for our questions. PDF document URL is https://arxiv.org/pdf/2403.05530.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Reference: https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.generative_models.Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import Part\n",
    "pdf_file_uri = \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\"\n",
    "pdf_file = Part.from_uri(pdf_file_uri, mime_type=\"application/pdf\")\n",
    "\n",
    "prompt = \"How many tokens can the model process?\"\n",
    "\n",
    "contents = [pdf_file, prompt]\n",
    "\n",
    "response = multimodal_model.generate_content(contents)\n",
    "display(IPython.display.Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "  You are a professional document summarization specialist.\n",
    "  Please summarize the given document.\n",
    "\"\"\"\n",
    "\n",
    "contents = [pdf_file, prompt]\n",
    "\n",
    "response = multimodal_model.generate_content(contents)\n",
    "display(IPython.display.Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usecase: Image Understanding across multiple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load images from the given url\n",
    "import http.client\n",
    "import typing\n",
    "import urllib.request\n",
    "from vertexai.generative_models import Image\n",
    "\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "        image_bytes = response.read()\n",
    "    return image_bytes\n",
    "\n",
    "\n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    image_bytes = get_image_bytes_from_url(image_url)\n",
    "    return Image.from_bytes(image_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_glasses1_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses1.jpg\"\n",
    "image_glasses2_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses2.jpg\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "I have an oval face. Given my face shape, which glasses would be more suitable?\n",
    "\n",
    "Explain how you reached this decision.\n",
    "Provide your recommendation based on my face shape, and please give an explanation for each.\n",
    "\"\"\"\n",
    "image_glasses1 = load_image_from_url(image_glasses1_url)\n",
    "image_glasses2 = load_image_from_url(image_glasses2_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Image(image_glasses1_url, width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Image(image_glasses2_url, width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = [prompt, image_glasses1, image_glasses2]\n",
    "responses = multimodal_model.generate_content(contents)\n",
    "display(IPython.display.Markdown(responses.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
