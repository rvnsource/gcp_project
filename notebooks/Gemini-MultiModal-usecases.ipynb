{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini-MultiModal-usecases.ipynb File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-requisites:\n",
    "    GCP account need to be created.\n",
    "    GCP project need to be linked with GCP billing/service account.\n",
    "    GCP project need to be linked with GCP service account key file.(json file)\n",
    "    Download json file and set env variable GOOGLE_APPLICATION_CREDENTIALS to the path of the downloaded file.\n",
    "    eg: export GOOGLE_APPLICATION_CREDENTIALS=\"/home/user/gcp_project/data/json/service-account-file.json\"\n",
    "    For different models,  ref. to model garden in Vertex AI in gcp console.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-cloud-aiplatform gitpython magika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "vertexai.init(project=\"river-span-431711-k8\", location=\"us-central1\")# Load the Gemini 1.5 Pro model. (https://cloud.google.com/vertex-ai/docs/reference/python/latest/vertexai.generative_models)\n",
    "\n",
    "# Load the Gemini 1.5 Pro model. (https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.generative_models)\n",
    "multimodal_model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "multimodal_model_flash = GenerativeModel(\"gemini-1.5-flash-001\") # using this var later\n",
    "\n",
    "# Generate response\n",
    "contents = [ \"Explain LLM\" ]\n",
    "response = multimodal_model.generate_content(contents)\n",
    "print(response)\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "display(IPython.display.Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gemini 1.5 pro model, we are going to process a PDF document. The model will analyze the document content, retain information, and provide answers for our questions. PDF document URL is https://arxiv.org/pdf/2403.05530.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Reference: https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.generative_models.Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import Part\n",
    "pdf_file_uri = \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\"\n",
    "pdf_file = Part.from_uri(pdf_file_uri, mime_type=\"application/pdf\")\n",
    "\n",
    "prompt = \"How many tokens can the model process?\"\n",
    "\n",
    "contents = [pdf_file, prompt]\n",
    "\n",
    "response = multimodal_model.generate_content(contents)\n",
    "display(IPython.display.Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "  You are a professional document summarization specialist.\n",
    "  Please summarize the given document.\n",
    "\"\"\"\n",
    "\n",
    "contents = [pdf_file, prompt]\n",
    "\n",
    "response = multimodal_model.generate_content(contents)\n",
    "display(IPython.display.Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usecase: Image Understanding across multiple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load images from the given url\n",
    "import http.client\n",
    "import typing\n",
    "import urllib.request\n",
    "from vertexai.generative_models import Image\n",
    "\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "        image_bytes = response.read()\n",
    "    return image_bytes\n",
    "\n",
    "\n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    image_bytes = get_image_bytes_from_url(image_url)\n",
    "    return Image.from_bytes(image_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_glasses1_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses1.jpg\"\n",
    "image_glasses2_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses2.jpg\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "I have an oval face. Given my face shape, which glasses would be more suitable?\n",
    "\n",
    "Explain how you reached this decision.\n",
    "Provide your recommendation based on my face shape, and please give an explanation for each.\n",
    "\"\"\"\n",
    "image_glasses1 = load_image_from_url(image_glasses1_url)\n",
    "image_glasses2 = load_image_from_url(image_glasses2_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Image(image_glasses1_url, width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Image(image_glasses2_url, width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = [prompt, image_glasses1, image_glasses2]\n",
    "responses = multimodal_model.generate_content(contents)\n",
    "display(IPython.display.Markdown(responses.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usecase: Generate description about a given video           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display content as video.\n",
    "def display_content_as_video(content: str | Image | Part):\n",
    "    if not isinstance(content, Part):\n",
    "        return False\n",
    "    part = typing.cast(Part, content)\n",
    "    file_path = part.file_data.file_uri.removeprefix(\"gs://\")\n",
    "    video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
    "    IPython.display.display(IPython.display.Video(video_url, width=350))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "display_content_as_video(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "What is shown in this video?\n",
    "Where should I go to see it?\n",
    "What are the top 5 places in the world that look like this?\n",
    "\"\"\"\n",
    "\n",
    "contents = [prompt, video]\n",
    "responses = multimodal_model.generate_content(contents)\n",
    "display(IPython.display.Markdown(responses.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/ottawatrain3.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "display_content_as_video(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Which line is this?\n",
    "Where does it go?\n",
    "What are the stations/stops?\n",
    "\"\"\"\n",
    "contents = [prompt, video]\n",
    "responses = multimodal_model.generate_content(contents)\n",
    "display(IPython.display.Markdown(responses.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use git codebase usecase\n",
    "### Given a git codebase, Undersand it and answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gitpython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# Helper functions to deal with git repo and source files.\n",
    "#############################################################\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import git\n",
    "import magika\n",
    "\n",
    "m = magika.Magika()\n",
    "\n",
    "def clone_repo(repo_url, repo_dir):\n",
    "    \"\"\"Clone a GitHub repository.\"\"\"\n",
    "    if os.path.exists(repo_dir):\n",
    "        shutil.rmtree(repo_dir)         # rm -rf <dir>\n",
    "\n",
    "    os.makedirs(repo_dir)\n",
    "    git.Repo.clone_from(repo_url, repo_dir)\n",
    "\n",
    "\n",
    "def extract_code(repo_dir):\n",
    "    \"\"\"Create code index and extract the content of source files from a GitHub repository.\"\"\"\n",
    "    code_index = []\n",
    "    code_text = \"\"\n",
    "    for root, dirs, files in os.walk(repo_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(file_path, repo_dir)\n",
    "            code_index.append(relative_path)\n",
    "\n",
    "            file_type = m.identify_path(Path(file_path))\n",
    "            if file_type.output.group in (\"text\", \"code\"):\n",
    "                try:\n",
    "                    with open(file_path, \"r\") as f:\n",
    "                        code_text += f\"----- File: {relative_path} -----\\n\"\n",
    "                        code_text += f.read()\n",
    "                        code_text += \"\\n-------------------------\\n\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    return code_index, code_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub repo url\n",
    "repo_url = \"https://github.com/GoogleCloudPlatform/microservices-demo\"\n",
    "\n",
    "# Location to clone the above git repo.\n",
    "repo_dir = \"./repo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the git repo and createa file index and extract contents of the code/text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_repo(repo_url, repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_index, code_text = extract_code(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to generate a prompt to a code related question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_prompt(question):\n",
    "    \"\"\"Generates a prompt to a code related question.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Questions: {question}\n",
    "\n",
    "    Context:\n",
    "    - The entire codebase is provided below.\n",
    "    - Here is an index of all of the files in the codebase:\n",
    "      \\n\\n{code_index}\\n\\n.\n",
    "    - Then each of the files is concatenated together. You will find all of the code you need:\n",
    "      \\n\\n{code_text}\\n\\n\n",
    "\n",
    "    Answer:\n",
    "  \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now create a prompt question.\n",
    "###### for eg: generate a \"Getting Started\" guide for new developers based on this project (code base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "  Provide a getting started guide to onboard new developers to the codebase.\n",
    "\"\"\"\n",
    "\n",
    "prompt = get_code_prompt(question)\n",
    "contents = [prompt]\n",
    "\n",
    "response = multimodal_model.generate_content(contents)\n",
    "IPython.display.Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate a Code Base Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "  Give me a summary of this codebase, and tell me the top 3 things that I can learn from it.\n",
    "\"\"\"\n",
    "\n",
    "prompt = get_code_prompt(question)\n",
    "contents = [prompt]\n",
    "\n",
    "# Generate text using non-streaming method\n",
    "response = multimodal_model_flash.generate_content(contents)\n",
    "IPython.display.Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the bugs in the code base (for some reason this is not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "  Find the top 3 most severe issues in the codebase.\n",
    "\"\"\"\n",
    "\n",
    "prompt = get_code_prompt(question)\n",
    "contents = [prompt]\n",
    "\n",
    "response = multimodal_model.generate_content(contents)\n",
    "IPython.display.Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
